<!DOCTYPE html>
<!--
    Plain-Academic by Vasilios Mavroudis
    Released under the Simplified BSD License/FreeBSD (2-clause) License.
    https://github.com/mavroudisv/plain-academic
-->

<html lang="en">

<head>
  <title>Hshmat Sahak</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css">
  <link rel="stylesheet" href="styles.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.0/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>
  <link href='https://fonts.googleapis.com/css?family=Oswald:700' rel='stylesheet' type='text/css'>
</head>

<body>

<!-- Navigation -->
<nav class="navbar navbar-inverse navbar-dark navbar-fixed-top" role="navigation">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                      <span class="sr-only">Toggle navigation</span>
                      <span class="icon-bar"></span>
                      <span class="icon-bar"></span>
                      <span class="icon-bar"></span>
        </button>
      </div>

      <!-- Collect the nav links, forms, and other content for toggling -->
      <div class="navbar-collapse collapse" id="bs-example-navbar-collapse-1">
        <ul class="nav navbar-nav navbar-right">
            <li class="nav-item"><a href="#about">About</a></li>
            <li class="nav-item"><a href="#education">Education</a></li>
            <li class="nav-item"><a href="#experience">Experience</a></li>
            <li class="nav-item"><a href="#publications">Publications</a></li> 
            <li class="nav-item"><a href="#projects">Projects</a></li>
            <li class="nav-item"><a href="Curriculum_Vitae.pdf" download="Hshmat_Sahak_CV">CV</a></li> 
        </ul>
      </div>
    </div>
</nav>
  
<!-- Page Content -->
<div class="container">

    <div>
        <div class="panel">
            <!--About-->
            <h1 id="about">Hshmat Sahak</h1>     
            <img class="img-responsive headshot" src="imgs/me.png" alt=""><br>       
            <p>I am a final-year MASc student at the University of Toronto (UofT), co-advised by <a href="http://asrl.utias.utoronto.ca/~tdb/" target="_blank" rel="noopener noreferrer">Tim Barfoot</a> and <a href="https://leaf.utias.utoronto.ca/author/nicholas-rhinehart/" target="_blank" rel="noopener noreferrer">Nick Rhinehart</a>. My MASc thesis revolves around perception and planning in indoor environments. I completed my BASc in Engineering Science at UofT, with a major in Machine Intelligence and a minor in Robotics & Mechatronics. My <a href="thesis.pdf" target="_blank" rel="noopener noreferrer">BASc thesis</a> was people detection in cluttered indoor environments.
            </p>
            
            <p>I am fortunate to have worked at great companies and research labs. I have previously worked at the <a href="https://vectorinstitute.ai/" target="_blank" rel="noopener noreferrer">Vector Institute</a> for AI 
                (advised by <a href="https://jimmylba.github.io/" target="_blank" rel="noopener noreferrer">Prof. Jimmy Ba</a>), <a href="https://research.google/teams/brain" target="_blank" rel="noopener noreferrer">Google Brain</a> (Student Researcher, advised by <a href="https://www.cs.toronto.edu/~fleet/" target="_blank" rel="noopener noreferrer">Prof. David Fleet</a>), 
                <a href="https://www.tesla.com/" target="_blank" rel="noopener noreferrer">Tesla</a> (Software Automation Engineer Intern) and <a href="https://www.nvidia.com/en-us/" target="_blank" rel="noopener noreferrer">Nvidia</a> 
                (Deep Learning Intern). At UofT, I have completed research internships at the <a href="https://d3m.mie.utoronto.ca/" target="_blank" rel="noopener noreferrer">Data-Driven Decision-Making Lab</a> 
                (advised by <a href="https://www.mie.utoronto.ca/faculty_staff/sanner/" target="_blank" rel="noopener noreferrer">Prof. Scott Sanner</a>) and the <a href="https://www.dynsyslab.org/vision-news/" target="_blank" rel="noopener noreferrer">Dynamic Systems Lab</a> 
                (advised by <a href="https://www.dynsyslab.org/prof-angela-schoellig/" target="_blank" rel="noopener noreferrer">Prof. Angela Schoellig</a>).
            </p>
            
            <p>My plan is to pursue a PhD within computer vision for better robot perception and/or planning algorithms for mobile robots.
            </p>
            
            <p>My specific research interests include:
                <ul>
                    <li>Robot Perception - 3D Object Detection and Semantic Segmentation for Scene Understanding</li>
                    <li>Autonomous Path Planning for Indoor and Outdoor Navigation of Robots</li>
                    <li>Developing efficient learning algorithms for function optimization in Deep Neural Networks</li>
                    <li>Generative AI for Image-to-Image and Discriminative tasks- e.g., Diffusion Models, GANs, NeRF Models</li>
                    <li>Broadly interested in Reinforcement Learning and NLP</li>
                    <li>Broadly interested in Computer Networks and Distributed Systems</li>
                </ul>
            </p>

            <p>
                If you are a current Graduate student in Machine Learning, Robotics, or at the intersection, I would love to hear about your work!
                If you are an incoming Engineering Science student, feel free to connect to ask questions about the program. You can reach me at 
                <a href="mailto:hshmat.sahak@mail.utoronto.ca" target="_blank" rel="noopener noreferrer">hshmat.sahak@mail.utoronto.ca</a>
            </p>

            <!-- <p><b>hshmat.sahak@mail.utoronto.ca</b><br>
            <p>University of Toronto<br>
            Undergraduate, 4th year<br>
            Major: Machine Learning<br>
            Minor: Robotics and Mechatronics <br>
            </p> -->
        </div>
        
        <!-- Links on the Sidebar -->
        <div class="panel" style="text-align: center;">
            <a href="https://scholar.google.com/citations?user=-L6_9fwAAAAJ&hl=en" target="_blank" rel="noopener noreferrer">Google Scholar</a> / <a href="https://twitter.com/HshmatSahak" target="_blank" rel="noopener noreferrer">Twitter</a> / <a href="https://www.linkedin.com/in/hshmat-sahak-08314b1b5" target="_blank" rel="noopener noreferrer">LinkedIn</a> / <a href="https://github.com/hshmatsahak" target="_blank" rel="noopener noreferrer">Github</a>
        </div>
    </div>
        
    <div>
        <div class="panel">
            <h2 id="education">Education</h2><br>
            <div class="image_text">
                <div class="featured_image">
                    <img src="imgs/uoft_logo.png" style="width: 100%;">
                </div>
                <div class="description">
                    <p style="vertical-align:middle;"><b>University of Toronto</b><br><br>
                        <b>MASc. in Aerospace Science and Engineering</b> | Sept 2024 - Aug 2026<br>
                        <a href="https://robotics.utoronto.ca/collaborative-specialization-in-robotics/" target="_blank" rel="noopener noreferrer" style="color: green">Collaborative Specialization in Robotics</a><br>
                        <b>Advisors:</b> <a href="http://asrl.utias.utoronto.ca/~tdb/" target="_blank" rel="noopener noreferrer">Tim Barfoot</a>, <a href="https://leaf.utias.utoronto.ca/author/nicholas-rhinehart/" target="_blank" rel="noopener noreferrer">Nick Rhinehart</a><br>
                        <b>Thesis:</b> Planning and navigation for indoor mobile robots<br>
                        <b>GPA:</b> 4.00/4.00<br>
                        <b>Relevant Courses:</b> State Estimation for Aerospace Vehicles, Imitation Learning for Robotics, AI Applications in Robotics, Mobile Robotics and Perception
                    </p>
                </div>
            </div><br>

            <div class="image_text">
                <div class="featured_image">
                    <img src="imgs/uoft_logo.png" style="width: 100%;">
                </div>
                <div class="description">
                    <p style="vertical-align:middle;"><b>University of Toronto</b><br><br>
                        <a href="https://engsci.utoronto.ca/program/what-is-engsci/" target="_blank" rel="noopener noreferrer" style="color: green">BASc. in Engineering Science</a> | Sept 2019 - Apr 2024<br>
                        <b>Machine Learning Major, Robotics and Mechatronics Minor</b><br>
                        <b>GPA:</b> 3.99/4.00<br>
                        <b>Relevant Courses:</b> Machine Learning, Robot Modelling and Control, Artificial Intelligence, Control Theory, 
                        Systems Software, Computing, Probabilistic Reasoning, Distributed Systems
                    </p>
                </div>
            </div>
        </div>

        <!--Work Experience-->
        <div class="panel">
            <h2 id="experience">Work Experience</h2><br>
            <div class="image_text">
                <div class="featured_image">
                    <img src="imgs/cerebras_logo.png" style="width: 100%;">
                </div>
                <div class="description">
                    <p style="vertical-align:middle;"><b>Applied ML Intern</b><br>
                        <a href="https://www.cerebras.net/" target="_blank" rel="noopener noreferrer" style="color: green">Cerebras Systems Inc.</a><br>
                        <i>May 2024 - Aug 2024 <span>&#183;</span> Toronto, ON, CA</i><br>
                        Expanded MathVista and MMMU datasets by creating augmented versions with similar samples, improving model performance
                        in multimodal math problem solving and visual reasoning. Compared retrieval methods using CLIP, DINO, and I-JEPA
                        embedding spaces, leading to higher scores across benchmarks including GQA, MathVista, MMMU, and VQA. Developed
                        long-context datasets using clustering techniques to optimize model performance on extended sequence length tasks.
                    </p>
                </div>
            </div><br>

            <div class="image_text">
                <div class="featured_image">
                    <img src="imgs/tesla_logo.jpeg" style="width: 100%;">
                </div>
                <div class="description">
                    <p style="vertical-align:middle;"><b>Software Automation Engineer Intern</b><br>
                        <a href="https://www.tesla.com/" target="_blank" rel="noopener noreferrer" style="color: green">Tesla</a><br>
                        <i>May 2023 - Aug 2023 <span>&#183;</span> Fremont, CA, USA</i><br>
                        Worked on Root Cause Analysis in the Cell Manufacturing Pipeline, and real-time Monitoring of Systems. Gained
                        familiarity with Prometheus, Grafana, InfluxDB, GoLang, React, Docker and Kubernetes. Applied ML and statistical 
                        techniques to anomaly detection and time series forecasting.
                    </p>
                </div>
            </div><br>

            <div class="image_text">
                <div class="featured_image">
                    <img src="imgs/vector_logo.jpeg" style="width: 100%;">
                </div>
                <div class="description">
                    <p style="vertical-align:middle;"><b>Research Intern</b><br>
                        <a href="https://vectorinstitute.ai/" target="_blank" rel="noopener noreferrer" style="color: green">Vector Institute</a><br>
                        <i>Jan 2023 - May 2023 <span>&#183;</span> Toronto, ON, CA</i><br>
                        Worked on using Generative AI, specifically Diffusion Models, to improve performance of discriminative models by training on 
                        real and synthetic images, instead of just the original dataset. Identified 3 key requirements in order for diffusion-based 
                        foundation models to outperform original datasets in downstream classification.
                    </p>
                </div>
            </div><br>

            <div class="image_text">
                <div class="featured_image">
                    <img src="imgs/google_logo.jpeg" style="width: 100%;">
                </div>
                <div class="description">
                    <p style="vertical-align:middle;"><b>Student Researcher</b><br>
                        <a href="https://research.google/teams/brain/" target="_blank" rel="noopener noreferrer" style="color: green">Google Brain</a><br>
                        <i>May 2022 - Dec 2022 <span>&#183;</span> Toronto, ON, CA</i><br>
                        Implemented Denoising Diffusion Probabilistic Models for Robust Super-Resolution (SR) in the wild. My work resulted in a patented
                        publication. The proposed model, SR3+, is an improvement over SR3 and can be used for photorealistic 4x SR even when
                        trained on out-of-distribution datasets, and can be cascaded for 16x SR.
                    </p>
                </div>
            </div><br>

            <div class="image_text">
                <div class="featured_image">
                    <img src="imgs/uoft_logo.png" style="width: 100%;">
                </div>
                <div class="description">
                    <p style="vertical-align:middle;"><b>Research Intern</b><br>
                        <a href="https://torontoheartcentre.ca/doctors/dr-paul-dorian/" target="_blank" rel="noopener noreferrer" style="color: green">University of Toronto, Division of Cardiology</a><br>
                        <i>Sep 2021 - Apr 2022 <span>&#183;</span> Toronto, ON, CA <span>&#183;</span> Part-Time</i><br>
                        I was the Lead Data Scientist in the Toronto Cardiology Team. I worked alongside a team of doctors; we co-authored 2 papers- 
                        both submitted to The 2022 Canadian Cardiovascular Congress. The project was to determine how patient-reported 
                        breathlessness correlates with several objective measures of respiration.  I analyzed data collected from exercise 
                        stress tests and produced visualizations using Python, MATLAB and Jupyter Notebook.
                    </p>
                </div>
            </div><br>

            <div class="image_text">
                <div class="featured_image">
                    <img src="imgs/nvidia_logo.png" style="width: 100%;">
                </div>
                <div class="description">
                    <p style="vertical-align:middle;"><b>Deep Learning for Power Architecture Intern</b><br>
                        <a href="https://www.nvidia.com/" target="_blank" rel="noopener noreferrer" style="color: green">Nvidia</a><br>
                        <i>May 2021 - Aug 2021 <span>&#183;</span> Santa Clara, CA, USA</i><br>
                        Worked on project using state-of-the-art machine learning techniques to enhance dynamic boost feature for next generation of 
                        Nvidia's GPU. Implemented algorithm to improve battery-mode dynamic compute power predictions by considering GPU idle time.
                    </p>
                </div>
            </div><br>

            <div class="image_text">
                <div class="featured_image">
                    <img src="imgs/uoft_logo.png" style="width: 100%;">
                </div>
                <div class="description">
                    <p style="vertical-align:middle;"><b>Research Intern</b><br>
                        <a href="https://d3m.mie.utoronto.ca/" target="_blank" rel="noopener noreferrer" style="color: green">University of Toronto, Data-Driven Decision-Making Lab</a><br>
                        <i>May 2021 - Aug 2021 <span>&#183;</span> Santa Clara, CA, USA <span>&#183;</span> Part-Time</i><br>
                        Investigated whether Twitter data was an appropriate indicator of the spread of Covid-19 across Canada and USA. I fine-tuned a 
                        BERT model (ML model for NLP) using PyTorch to detect mask sentiments, resulting in a 41% improvement in mask attitude classification 
                        compared to vanilla VADER sentiment analysis, outperforming existing hashtag and regex-based classifiers
                    </p>
                </div>
            </div><br>

            <div class="image_text">
                <div class="featured_image">
                    <img src="imgs/dsl_logo.png" style="width: 100%;">
                </div>
                <div class="description">
                    <p style="vertical-align:middle;"><b>Research Intern</b><br>
                        <a href="https://www.dynsyslab.org/vision-news/" target="_blank" rel="noopener noreferrer" style="color: green">University of Toronto, Dynamic Systems Lab</a><br>
                        <i>May 2020 - Aug 2020 <span>&#183;</span> Santa Clara, CA, USA <span>&#183;</span> Part-Time</i><br>
                        Designed and implement a scalable, real time trajectory generation algorithm using MATLAB to synchronize the flight of 50 drones
                        with live music from a MIDI keyboard. I also surveyed 20+ safe learning research papers and categorized them by key ML concepts
                        to aid in the creation of a safe learning survey paper.
                    </p>
                </div>
            </div><br>

            <div class="image_text">
                <div class="featured_image">
                    <img src="imgs/sunnybrook_logo.jpeg" style="width: 100%;">
                </div>
                <div class="description">
                    <p style="vertical-align:middle;"><b>Software Intern</b><br>
                        <a href="https://sunnybrook.ca/research/content/?page=sri-groups-fus" target="_blank" rel="noopener noreferrer" style="color: green">Sunnybrook Research Institute</a><br>
                        <i>May 2020 - Aug 2020 <span>&#183;</span> Santa Clara, CA, USA <span>&#183;</span> Part-Time</i><br>
                        Part of the Focused Ultrasound High School Summer Research Program. I implemented an algorithm using Fast
                        Fourier transform to identify distribution of harmonics present in ultrasound-induced motion signals. I stored
                        lesion observations in a patient-to-observation database using SQL. I visualized lesion observations as a pseudo-colour, 
                        allowing for customizing colour pixel and bit representation using MATLAB.
                    </p>
                </div>
            </div><br>
        </div>        
                
        <!--Publications-->
        <div class="panel">    
            <h2 id="publications">Publications</h2>
            <div class="image_text">
                <div class="featured_image">
                    <img src="imgs/ratatouille.png" style="width: 100%;">
                </div>
                <div class="description">
                    <p style="vertical-align:middle;">
                        <strong style="color:blue;">Ratatouille: Imitation Learning Ingredients for Real-world Social Robot Navigation</strong><br>
                        <a href="https://www.linkedin.com/in/james-han-9b686a1a8/" target="_blank" rel="noopener noreferrer">James R. Han</a>, Mithun Vanniasinghe, <strong>Hshmat Sahak</strong>, <a href="https://leaf.utias.utoronto.ca/author/nicholas-rhinehart/" target="_blank" rel="noopener noreferrer">Nicholas Rhinehart</a>, <a href="http://asrl.utias.utoronto.ca/~tdb/" target="_blank" rel="noopener noreferrer">Timothy D. Barfoot</a><br>
                        <a href="https://arxiv.org/abs/2509.17204" target="_blank" rel="noopener noreferrer" style="color:purple">Paper</a> / <a href="https://youtu.be/tOdLTXsaYLQ" target="_blank" rel="noopener noreferrer" style="color:purple">Video</a><br><br>
                        We present Ratatouille, a behavior cloning pipeline for social robot navigation that integrates weight perturbations, data augmentation,
                        hybrid action spaces, action chunking, and neural network optimization techniques. Without changing the data, Ratatouille reduces
                        collisions per meter by 6× and improves success rate by 3× compared to naïve behavior cloning. We validate our approach in both
                        simulation and the real world, collecting over 11 hours of data on a dense university campus and demonstrating qualitative results
                        in a public food court.
                    </p>
                </div>
            </div><br>

            <div class="image_text">
                <div class="featured_image">
                    <img src="imgs/recipevis.png" style="width: 100%;">
                </div>
                <div class="description">
                    <p style="vertical-align:middle;">
                        <strong style="color:blue;">RecipeVis: Fusing Vision and Language Models to Generate Sequence of Recipe Images from Steps</strong><br>
                        <strong>Hshmat Sahak</strong><br>
                        <a href="https://openreview.net/forum?id=McFvoo7s6V" target="_blank" rel="noopener noreferrer" style="color:purple">Paper</a><br><br>
                        We present RecipeVis, which generates an image for each step in a recipe by conditioning on the previously generated image and current step.
                        RecipeVis leverages the pretrained text-to-image Stable Diffusion model, as well as text and visual encoders that produce task-agnostic
                        embeddings. It uses an attention module to fuse text and image embeddings, and adds a cycle consistency loss to the standard diffusion
                        loss to ensure consistency between output modes. Our empirical study demonstrates that vision-language models incorporating previous
                        images provide superior results over baseline text-to-image models.
                    </p>
                </div>
            </div><br>

            <div class="image_text">
                <div class="featured_image">
                    <img src="imgs/method.png" style="width: 100%;">
                </div>
                <div class="description">
                    <p style="vertical-align:middle;">
                        <strong style="color:blue;">Training on Thin Air: Improve Image Classification with Generated Data</strong><br>
                        <a href="https://www.linkedin.com/in/yongchao-zhou-a298a7158/?originalSubdomain=ca" target="_blank" rel="noopener noreferrer">Yongchao Zhou</a>, <strong>Hshmat Sahak</strong>, <a href="https://jimmylba.github.io/" target="_blank" rel="noopener noreferrer">Jimmy Ba</a><br>
                        <a style="color:purple" target="_blank" rel="noopener noreferrer">Project Page</a> / <a style="color:purple" target="_blank" rel="noopener noreferrer">Paper</a> / <a target="_blank" rel="noopener noreferrer" style="color:purple">Code</a> / <a target="_blank" rel="noopener noreferrer" style="color:purple">Slides</a><br><br>
                        We present Diffusion Inversion, a simple yet effective method that leverages the pre-trained generative model, Stable Diffusion,
                        to generate diverse, high-quality training data for image classification. Our approach captures the original data distribution
                        and ensures data coverage by inverting images to the latent space of Stable Diffusion, and generates diverse novel training images 
                        by conditioning the generative model on noisy versions of these vectors.
                    </p>
                </div>
            </div><br>

            <div class="image_text">
                <div class="featured_image">
                    <!-- <img src="imgs/method.png" style="float: left; margin-right: 15px;" width="175" height="175"> -->
                    <video id="vid" width="100%" autoplay muted>
                        <source src="sr3p.mp4" type="video/mp4">
                    </video>
                    <script>
                        document.getElementById('vid').play();
                    </script>
                </div>
                <div class="description">
                    <p style="vertical-align:middle;">
                        <strong style="color:blue;">Denoising Diffusion Probabilistic Models for Robust Image Super-Resolution in the Wild</strong><br>
                        <strong>Hshmat Sahak</strong>, <a href="https://www.linkedin.com/in/danielwatson6/" target="_blank" rel="noopener noreferrer">Daniel Watson</a>, <a href="https://www.linkedin.com/in/chitwansaharia/?originalSubdomain=ca" target="_blank" rel="noopener noreferrer">Chitwan Saharia</a>, <a href="https://www.cs.toronto.edu/~fleet/" target="_blank" rel="noopener noreferrer">David Fleet</a><br>
                        <a href="https://arxiv.org/abs/2302.07864" target="_blank" rel="noopener noreferrer" style="color:purple">Paper</a><br><br>
                        This paper introduces SR3+, a diffusion-based model for blind super-resolution, establishing a new state-of-the-art. We advocate 
                        self-supervised training with a combination of composite, parameterized degradations for self-supervised training, and noise-conditioing 
                        augmentation during training and testing.
                    </p>
                </div>
            </div><br>
        </div>

        <!--Projects-->
        <div class="panel">
            <h2 id="projects">Projects</h2><br>
            <div class="image_text">
                <div class="featured_image">
                    <img src="imgs/projects/method.png" style="width: 100%;">
                </div>
                <div class="description">
                    <p style="vertical-align:middle;"><b>Diffusion Inversion: Steering Pre-Trained Diffusion Models to Improve Image Classification on Rare Datasets</b><br>
                        [Research Project] @ Vector Institute, with Yongchao Zhou and Jimmy Ba.<br><br>
                        We propose Diffusion Inversion, a simple yet effective method that utilizes pre-trained generative 
                        models to assist with discriminative learning, bridging the gap between real and synthetic data. 
                        Our method offers 2-3x sample complexity improvements and 6.5x reduction in sampling time.
                    </p>
                </div>
            </div><br>

            <div class="image_text">
                <div class="featured_image">
                    <img src="imgs/projects/blueguardian.jpeg" style="width: 100%;">
                </div>
                <div class="description">
                    <p style="vertical-align:middle;"><b>Blue Guardian: Protect Before they Connect</b><br>
                        [Research Project] @ Blue Guardian<br><br>
                        Blue Guardian is a startup that aims to revolutionize Youth Mental Health Detection through Sentiment Analysis of text messages.
                        I worked on implementing state-of-the-art NLP methods for emotion detection, including feature extraction and transformer architecture
                        implementation. I also tackled problems of limited and imbalanced datasets resulting from uncommon emotions. Our model has been deployed 
                        and can be accessed through the app or <a href="#" target="_blank" rel="noopener noreferrer">website</a>.
                    </p>
                </div>
            </div><br>

            <div class="image_text">
                <div class="featured_image">
                    <!-- <img src="imgs/method.png" style="float: left; margin-right: 15px;" width="175" height="175"> -->
                    <video id="vid" width="100%" autoplay muted>
                        <source src="sr3p.mp4" type="video/mp4">
                    </video>
                    <script>
                        document.getElementById('vid').play();
                    </script>
                </div>
                <div class="description">
                    <p style="vertical-align:middle;">
                        <b>SR3+: Using DDPMs for Robust Super-Resolution in the Wild</b><br>
                        [Research Project] @ Google Brain, advised by David Fleet.<br><br>
                        We introduce SR3+, a diffusion model for blind image super-resolution, outperforming SR3 and the previous
                        SOTA on zero-shot RealSR and DRealSR benchmarks, across different model and training set sizes. Through a 
                        careful ablation study, we demonstrate the complementary benefits of parametric degradations and
                        noise conditioning augmentation techniques (with the latter also used at test time).
                    </p>
                </div>
            </div><br>

            <div class="image_text">
                <div class="featured_image">
                    <img src="imgs/projects/vghnet.png" style="width: 100%;">
                </div>
                <div class="description">
                    <p style="vertical-align:middle;"><b>VGH-Net: Predict Restaurant Ratings from Food Images using CNN</b><br>
                        [Final Project] @ University of Toronto, ECE324: Machine Learning, Software, and Neural Networks<br>
                        <a href="#" target="_blank" rel="noopener noreferrer">Final Report</a> / <a target="_blank" rel="noopener noreferrer">Code</a><br><br>
                        We present VGH-Net; a novel architecture for predicting restaurant ratings from a set of food images. VGH-Net training occurs 
                        in two stages. First, a CNN learns to predict image embeddings matching the output of BERT on a restaurant's text reviews. 
                        Then, it attaches a feed-forward neural network to predict the review-specific restaurant rating from the embedding.
                    </p>
                </div>
            </div><br>

            <div class="image_text">
                <div class="featured_image">
                    <img src="imgs/projects/praxis3.png" style="width: 100%;">
                </div>
                <div class="description">
                    <p style="vertical-align:middle;"><b>Trash n' Track : Addressing Waste Management in Ghana</b><br>
                        [Final Project] @ University of Toronto, ESC204: Praxis III<br>
                        <a href="#" target="_blank" rel="noopener noreferrer">Final Report</a> / <a href="#" target="_blank" rel="noopener noreferrer">Slides</a><br><br>
                        Trash n' Track is a modular system that can be attached to existing waste bins to let waste collectors know of the location of full bins. It
                        uses an ultrasonic sensor to determine the available capacity of the waste bin. This information, along with the location of the waste 
                        bin determined from an onboard GPS module, is sent to a server using a LoRa transceiver. We have also created a webapp that connects to 
                        this backend in order to provide live updates of waste bin fullness and location to waste collectors.                            
                    </p>
                </div>
            </div><br>

            <div class="image_text">
                <div class="featured_image">
                    <img src="imgs/projects/d3m.png" style="width: 100%;">
                </div>
                <div class="description">
                    <p style="vertical-align:middle;"><b>Using Twitter to detect the Spread of Covid-19</b><br>
                        [Research Project] @ University of Toronto, Data-Driven Decision-Making Lab<br>
                        <a href="#" target="_blank" rel="noopener noreferrer">Code</a> / <a href="#" target="_blank" rel="noopener noreferrer">Slides</a><br><br>
                        Compared COVID statistics in various regions with corresponding COVID-related tweet counts and mask/vaccine hesitancy scores. Used Folium library, 
                        word clouds and topic modelling to identify spatial & temporal tweet distribution. For more accurate labelling, fine-tuned BERT model using PyTorch 
                        to detect mask sentiments, improving mask attitude classification by 41% compared to vanilla VADER sentiment analysis and outperforming existing hashtag & regex-based classifiers                  
                    </p>
                </div>
            </div><br>

            <div class="image_text">
                <div class="featured_image">
                    <img src="imgs/projects/chessai.png" style="width: 100%;">
                </div>
                <div class="description">
                    <p style="vertical-align:middle;"><b>Chess AI</b><br>
                        [Side Project], with Ritvik Singh<br>
                        <a href="#" target="_blank" rel="noopener noreferrer">Code</a><br><br>
                        Developed chess engine using Python chess module. Beta release used alpha-beta pruning on a minimax tree. Final version 
                        uses multiple advanced algorithms, including Killer Move Heuristic, History Heuristic, Principle Variation Search, Transposition Table, 
                        Zobrist Hashing, Quiescent Search, and Iterative Deepening. The AI is around 1500 ELO as measured by chess.com. 
                    </p>
                </div>
            </div><br>

            <div class="image_text">
                <div class="featured_image">
                    <img src="imgs/projects/snek.png" style="width: 100%;">
                </div>
                <div class="description">
                    <p style="vertical-align:middle;"><b>Snake Game</b><br>
                        [Final Project] @ University of Toronto, ESC190: Data Structures and Algorithms<br>
                        <a href="#" target="_blank" rel="noopener noreferrer">Final Report</a> / <a href="#" target="_blank" rel="noopener noreferrer">Code</a><br><br>
                        Developed an AI agent to play the snake game. The agent starts at the top left and aims to consume all food while avoiding 
                        hitting the walls and itself as it grows. For small board sizes, used Randomized Path Finding Algorithm. For larger board sizes, 
                        used DFS on the game state stack to find the target. It is flexible across varying board sizes, while keeping O(n) 
                        space complexity on an nxn board. 
                    </p>
                </div>
            </div><br>

            <div class="image_text">
                <div class="featured_image">
                    <img src="imgs/projects/kohai.png" style="width: 100%;">
                </div>
                <div class="description">
                    <p style="vertical-align:middle;"><b>FRONT: Improved Can Openers at Kohai Life</b><br>
                        [Final Project] @ University of Toronto, ESC102: Praxis II, with Mingde Yin, Aoran Jiao, Ryan Ghosh<br>
                        <a href="#" target="_blank" rel="noopener noreferrer">Report</a> / <a href="#" target="_blank" rel="noopener noreferrer">Demo</a><br><br>
                        FRONT: Force Redirection Object Negotiator Tool, a retrofit extension designed to
                        be attached to the actuator bar of the handle of a can opener to allow opening of cans with less directed
                        work and make it easier for people of differing abilities to use in daily life. FRONT is a retrofit module
                        that aims to convert a simpler pulling force in any direction to the traditional rotary force needed to cut a
                        can. The nature of the design is intuitive; to cut the can one simply attaches the module and pulls on a
                        string attached to a rotary guide wheel, guaranteeing that there is some component of force in the
                        direction of undisturbed motion(useful work)           
                    </p>
                </div>
            </div><br>

            <div class="image_text">
                <div class="featured_image">
                    <img src="imgs/projects/robot.jpeg" style="width: 100%;">
                </div>
                <div class="description">
                    <p style="vertical-align:middle;"><b>Robot Pet</b><br>
                        [Side Project], with Gabriel Luo<br>
                        <a href="#" target="_blank" rel="noopener noreferrer">Code</a> / <a href="#" target="_blank" rel="noopener noreferrer">Demo</a><br><br>
                        Built and programmed a robotic pet that can pick up and sort objects, respond to stimuli, draw, and play the piano.
                        Worked with Arduino microcontroller; using Bluetooth to control the arm and various sensors for obstacle detection, path planning
                        and environmental detection.
                    </p>
                </div>
            </div><br>
        </div><br>
    </div>
</div>

<!--Interests/Hobbies-->
<!-- <div style="padding-bottom: 100px;" class="col-md-8 col-md-offset-2">
    <h2 style="text-align:center;">Interests/Hobbies</h2><br>
    <div class="gallery"> 
        <div class="gallery-item">
            <img class="gallery-image" src="imgs/hobbies/art.png"><br>
            <p style="text-align:center">I enjoy art (drawing and painting). As you can tell, I'm not very good at it. I also like playing basketball, which falls as art because it is poetry in motion.
            Most art I produce ends up being "abstract", but not intentionally.</p>
        </div>

        <div class="gallery-item">
            <img class="gallery-image" src="imgs/hobbies/chess.jpeg">
            <p style="text-align:center">I enjoy playing chess. I have participated in several team chess tournaments in high school, and have continued playing as a hobby through Undergrad.
            My peak rating was 1710. Access my chess.com profile <a href="https://www.chess.com/member/sickmanballer" target="_blank" rel="noopener noreferrer">here</a></p>
        </div>

        <div class="gallery-item">
            <img class="gallery-image" src="imgs/hobbies/rotary.jpeg"><br>
            <p style="text-align:center">I have received several scholarships for my volunteer work. I have volunteered at many sites since middle school (over 1000 recorded volunteer hours), including the 
                Retirement Center and Tutoring Program. </p>
        </div>
    </div>
</div> -->


</div>
    
</body>

</html>
